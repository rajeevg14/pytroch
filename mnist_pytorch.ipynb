{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cn1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.dp1 = nn.Dropout2d(0.10)\n",
    "        self.dp2 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(4608, 64) # 4608 is basically 12 X 12 X 32\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.cn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dp1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp2(x)\n",
    "        x = self.fc2(x)\n",
    "        op = F.log_softmax(x, dim=1)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define training and inference routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_dataloader, optim, epoch):\n",
    "    model.train()\n",
    "    for b_i, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred_prob = model(X)\n",
    "        loss = F.nll_loss(pred_prob, y) # nll is the negative likelihood loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if b_i % 10 == 0:\n",
    "            print('epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'.format(\n",
    "                epoch, b_i * len(X), len(train_dataloader.dataset),\n",
    "                100. * b_i / len(train_dataloader), loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    success = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_prob = model(X)\n",
    "            loss += F.nll_loss(pred_prob, y, reduction='sum').item()  # loss summed across the batch\n",
    "            pred = pred_prob.argmax(dim=1, keepdim=True)  # us argmax to get the most likely prediction\n",
    "            success += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    loss /= len(test_dataloader.dataset)\n",
    "\n",
    "    print('\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss, success, len(test_dataloader.dataset),\n",
    "        100. * success / len(test_dataloader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 9912422/9912422 [00:00<00:00, 11329013.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 28881/28881 [00:00<00:00, 7867486.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 1648877/1648877 [00:00<00:00, 10212042.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 4542/4542 [00:00<00:00, 6266621.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The mean and standard deviation values are calculated as the mean of all pixel values of all images in the training dataset\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,))])), # train_X.mean()/256. and train_X.std()/256.\n",
    "    batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,)) \n",
    "                   ])),\n",
    "    batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define optimizer and run training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = ConvNet()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajeev/anaconda3/lib/python3.10/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.301478\n",
      "epoch: 1 [320/60000 (1%)]\t training loss: 1.744134\n",
      "epoch: 1 [640/60000 (1%)]\t training loss: 1.240408\n",
      "epoch: 1 [960/60000 (2%)]\t training loss: 0.842911\n",
      "epoch: 1 [1280/60000 (2%)]\t training loss: 0.949036\n",
      "epoch: 1 [1600/60000 (3%)]\t training loss: 0.558535\n",
      "epoch: 1 [1920/60000 (3%)]\t training loss: 0.620242\n",
      "epoch: 1 [2240/60000 (4%)]\t training loss: 0.507009\n",
      "epoch: 1 [2560/60000 (4%)]\t training loss: 0.467790\n",
      "epoch: 1 [2880/60000 (5%)]\t training loss: 0.332347\n",
      "epoch: 1 [3200/60000 (5%)]\t training loss: 0.371715\n",
      "epoch: 1 [3520/60000 (6%)]\t training loss: 0.364097\n",
      "epoch: 1 [3840/60000 (6%)]\t training loss: 0.332301\n",
      "epoch: 1 [4160/60000 (7%)]\t training loss: 0.452664\n",
      "epoch: 1 [4480/60000 (7%)]\t training loss: 0.365426\n",
      "epoch: 1 [4800/60000 (8%)]\t training loss: 0.460753\n",
      "epoch: 1 [5120/60000 (9%)]\t training loss: 0.203205\n",
      "epoch: 1 [5440/60000 (9%)]\t training loss: 0.311852\n",
      "epoch: 1 [5760/60000 (10%)]\t training loss: 0.120139\n",
      "epoch: 1 [6080/60000 (10%)]\t training loss: 0.348500\n",
      "epoch: 1 [6400/60000 (11%)]\t training loss: 0.240864\n",
      "epoch: 1 [6720/60000 (11%)]\t training loss: 0.086965\n",
      "epoch: 1 [7040/60000 (12%)]\t training loss: 0.311653\n",
      "epoch: 1 [7360/60000 (12%)]\t training loss: 0.069089\n",
      "epoch: 1 [7680/60000 (13%)]\t training loss: 0.228319\n",
      "epoch: 1 [8000/60000 (13%)]\t training loss: 0.209495\n",
      "epoch: 1 [8320/60000 (14%)]\t training loss: 0.335217\n",
      "epoch: 1 [8640/60000 (14%)]\t training loss: 0.088607\n",
      "epoch: 1 [8960/60000 (15%)]\t training loss: 0.077267\n",
      "epoch: 1 [9280/60000 (15%)]\t training loss: 0.299731\n",
      "epoch: 1 [9600/60000 (16%)]\t training loss: 0.217775\n",
      "epoch: 1 [9920/60000 (17%)]\t training loss: 0.224393\n",
      "epoch: 1 [10240/60000 (17%)]\t training loss: 0.202607\n",
      "epoch: 1 [10560/60000 (18%)]\t training loss: 0.534872\n",
      "epoch: 1 [10880/60000 (18%)]\t training loss: 0.091350\n",
      "epoch: 1 [11200/60000 (19%)]\t training loss: 0.164567\n",
      "epoch: 1 [11520/60000 (19%)]\t training loss: 0.305187\n",
      "epoch: 1 [11840/60000 (20%)]\t training loss: 0.055809\n",
      "epoch: 1 [12160/60000 (20%)]\t training loss: 0.157737\n",
      "epoch: 1 [12480/60000 (21%)]\t training loss: 0.047026\n",
      "epoch: 1 [12800/60000 (21%)]\t training loss: 0.132636\n",
      "epoch: 1 [13120/60000 (22%)]\t training loss: 0.145910\n",
      "epoch: 1 [13440/60000 (22%)]\t training loss: 0.079548\n",
      "epoch: 1 [13760/60000 (23%)]\t training loss: 0.057228\n",
      "epoch: 1 [14080/60000 (23%)]\t training loss: 0.036533\n",
      "epoch: 1 [14400/60000 (24%)]\t training loss: 0.147256\n",
      "epoch: 1 [14720/60000 (25%)]\t training loss: 0.132342\n",
      "epoch: 1 [15040/60000 (25%)]\t training loss: 0.087954\n",
      "epoch: 1 [15360/60000 (26%)]\t training loss: 0.199888\n",
      "epoch: 1 [15680/60000 (26%)]\t training loss: 0.228492\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 0.152473\n",
      "epoch: 1 [16320/60000 (27%)]\t training loss: 0.080728\n",
      "epoch: 1 [16640/60000 (28%)]\t training loss: 0.041752\n",
      "epoch: 1 [16960/60000 (28%)]\t training loss: 0.116074\n",
      "epoch: 1 [17280/60000 (29%)]\t training loss: 0.051243\n",
      "epoch: 1 [17600/60000 (29%)]\t training loss: 0.073239\n",
      "epoch: 1 [17920/60000 (30%)]\t training loss: 0.040978\n",
      "epoch: 1 [18240/60000 (30%)]\t training loss: 0.021529\n",
      "epoch: 1 [18560/60000 (31%)]\t training loss: 0.074976\n",
      "epoch: 1 [18880/60000 (31%)]\t training loss: 0.302484\n",
      "epoch: 1 [19200/60000 (32%)]\t training loss: 0.146732\n",
      "epoch: 1 [19520/60000 (33%)]\t training loss: 0.156637\n",
      "epoch: 1 [19840/60000 (33%)]\t training loss: 0.089812\n",
      "epoch: 1 [20160/60000 (34%)]\t training loss: 0.086282\n",
      "epoch: 1 [20480/60000 (34%)]\t training loss: 0.077581\n",
      "epoch: 1 [20800/60000 (35%)]\t training loss: 0.084695\n",
      "epoch: 1 [21120/60000 (35%)]\t training loss: 0.348953\n",
      "epoch: 1 [21440/60000 (36%)]\t training loss: 0.101311\n",
      "epoch: 1 [21760/60000 (36%)]\t training loss: 0.152512\n",
      "epoch: 1 [22080/60000 (37%)]\t training loss: 0.053798\n",
      "epoch: 1 [22400/60000 (37%)]\t training loss: 0.205589\n",
      "epoch: 1 [22720/60000 (38%)]\t training loss: 0.228144\n",
      "epoch: 1 [23040/60000 (38%)]\t training loss: 0.036137\n",
      "epoch: 1 [23360/60000 (39%)]\t training loss: 0.256782\n",
      "epoch: 1 [23680/60000 (39%)]\t training loss: 0.358061\n",
      "epoch: 1 [24000/60000 (40%)]\t training loss: 0.069725\n",
      "epoch: 1 [24320/60000 (41%)]\t training loss: 0.060854\n",
      "epoch: 1 [24640/60000 (41%)]\t training loss: 0.192327\n",
      "epoch: 1 [24960/60000 (42%)]\t training loss: 0.066712\n",
      "epoch: 1 [25280/60000 (42%)]\t training loss: 0.030303\n",
      "epoch: 1 [25600/60000 (43%)]\t training loss: 0.317986\n",
      "epoch: 1 [25920/60000 (43%)]\t training loss: 0.531736\n",
      "epoch: 1 [26240/60000 (44%)]\t training loss: 0.031080\n",
      "epoch: 1 [26560/60000 (44%)]\t training loss: 0.043352\n",
      "epoch: 1 [26880/60000 (45%)]\t training loss: 0.229293\n",
      "epoch: 1 [27200/60000 (45%)]\t training loss: 0.053658\n",
      "epoch: 1 [27520/60000 (46%)]\t training loss: 0.012724\n",
      "epoch: 1 [27840/60000 (46%)]\t training loss: 0.127009\n",
      "epoch: 1 [28160/60000 (47%)]\t training loss: 0.007836\n",
      "epoch: 1 [28480/60000 (47%)]\t training loss: 0.171291\n",
      "epoch: 1 [28800/60000 (48%)]\t training loss: 0.102446\n",
      "epoch: 1 [29120/60000 (49%)]\t training loss: 0.493341\n",
      "epoch: 1 [29440/60000 (49%)]\t training loss: 0.014101\n",
      "epoch: 1 [29760/60000 (50%)]\t training loss: 0.071718\n",
      "epoch: 1 [30080/60000 (50%)]\t training loss: 0.399044\n",
      "epoch: 1 [30400/60000 (51%)]\t training loss: 0.039880\n",
      "epoch: 1 [30720/60000 (51%)]\t training loss: 0.101556\n",
      "epoch: 1 [31040/60000 (52%)]\t training loss: 0.075627\n",
      "epoch: 1 [31360/60000 (52%)]\t training loss: 0.053511\n",
      "epoch: 1 [31680/60000 (53%)]\t training loss: 0.028118\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 0.002929\n",
      "epoch: 1 [32320/60000 (54%)]\t training loss: 0.078423\n",
      "epoch: 1 [32640/60000 (54%)]\t training loss: 0.211911\n",
      "epoch: 1 [32960/60000 (55%)]\t training loss: 0.005935\n",
      "epoch: 1 [33280/60000 (55%)]\t training loss: 0.481908\n",
      "epoch: 1 [33600/60000 (56%)]\t training loss: 0.074895\n",
      "epoch: 1 [33920/60000 (57%)]\t training loss: 0.049652\n",
      "epoch: 1 [34240/60000 (57%)]\t training loss: 0.042982\n",
      "epoch: 1 [34560/60000 (58%)]\t training loss: 0.005306\n",
      "epoch: 1 [34880/60000 (58%)]\t training loss: 0.075362\n",
      "epoch: 1 [35200/60000 (59%)]\t training loss: 0.119033\n",
      "epoch: 1 [35520/60000 (59%)]\t training loss: 0.103477\n",
      "epoch: 1 [35840/60000 (60%)]\t training loss: 0.049465\n",
      "epoch: 1 [36160/60000 (60%)]\t training loss: 0.012482\n",
      "epoch: 1 [36480/60000 (61%)]\t training loss: 0.071684\n",
      "epoch: 1 [36800/60000 (61%)]\t training loss: 0.164245\n",
      "epoch: 1 [37120/60000 (62%)]\t training loss: 0.214892\n",
      "epoch: 1 [37440/60000 (62%)]\t training loss: 0.119813\n",
      "epoch: 1 [37760/60000 (63%)]\t training loss: 0.012564\n",
      "epoch: 1 [38080/60000 (63%)]\t training loss: 0.119609\n",
      "epoch: 1 [38400/60000 (64%)]\t training loss: 0.123996\n",
      "epoch: 1 [38720/60000 (65%)]\t training loss: 0.279660\n",
      "epoch: 1 [39040/60000 (65%)]\t training loss: 0.018233\n",
      "epoch: 1 [39360/60000 (66%)]\t training loss: 0.273742\n",
      "epoch: 1 [39680/60000 (66%)]\t training loss: 0.048702\n",
      "epoch: 1 [40000/60000 (67%)]\t training loss: 0.080287\n",
      "epoch: 1 [40320/60000 (67%)]\t training loss: 0.056409\n",
      "epoch: 1 [40640/60000 (68%)]\t training loss: 0.156436\n",
      "epoch: 1 [40960/60000 (68%)]\t training loss: 0.140869\n",
      "epoch: 1 [41280/60000 (69%)]\t training loss: 0.297263\n",
      "epoch: 1 [41600/60000 (69%)]\t training loss: 0.170574\n",
      "epoch: 1 [41920/60000 (70%)]\t training loss: 0.019871\n",
      "epoch: 1 [42240/60000 (70%)]\t training loss: 0.008991\n",
      "epoch: 1 [42560/60000 (71%)]\t training loss: 0.017744\n",
      "epoch: 1 [42880/60000 (71%)]\t training loss: 0.101491\n",
      "epoch: 1 [43200/60000 (72%)]\t training loss: 0.041510\n",
      "epoch: 1 [43520/60000 (73%)]\t training loss: 0.151213\n",
      "epoch: 1 [43840/60000 (73%)]\t training loss: 0.109483\n",
      "epoch: 1 [44160/60000 (74%)]\t training loss: 0.144559\n",
      "epoch: 1 [44480/60000 (74%)]\t training loss: 0.061091\n",
      "epoch: 1 [44800/60000 (75%)]\t training loss: 0.304672\n",
      "epoch: 1 [45120/60000 (75%)]\t training loss: 0.099090\n",
      "epoch: 1 [45440/60000 (76%)]\t training loss: 0.169923\n",
      "epoch: 1 [45760/60000 (76%)]\t training loss: 0.011046\n",
      "epoch: 1 [46080/60000 (77%)]\t training loss: 0.440497\n",
      "epoch: 1 [46400/60000 (77%)]\t training loss: 0.086487\n",
      "epoch: 1 [46720/60000 (78%)]\t training loss: 0.176612\n",
      "epoch: 1 [47040/60000 (78%)]\t training loss: 0.101528\n",
      "epoch: 1 [47360/60000 (79%)]\t training loss: 0.100468\n",
      "epoch: 1 [47680/60000 (79%)]\t training loss: 0.090491\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 0.124091\n",
      "epoch: 1 [48320/60000 (81%)]\t training loss: 0.201857\n",
      "epoch: 1 [48640/60000 (81%)]\t training loss: 0.009461\n",
      "epoch: 1 [48960/60000 (82%)]\t training loss: 0.011842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [49280/60000 (82%)]\t training loss: 0.182928\n",
      "epoch: 1 [49600/60000 (83%)]\t training loss: 0.069349\n",
      "epoch: 1 [49920/60000 (83%)]\t training loss: 0.010657\n",
      "epoch: 1 [50240/60000 (84%)]\t training loss: 0.096470\n",
      "epoch: 1 [50560/60000 (84%)]\t training loss: 0.003839\n",
      "epoch: 1 [50880/60000 (85%)]\t training loss: 0.006649\n",
      "epoch: 1 [51200/60000 (85%)]\t training loss: 0.236572\n",
      "epoch: 1 [51520/60000 (86%)]\t training loss: 0.014727\n",
      "epoch: 1 [51840/60000 (86%)]\t training loss: 0.024024\n",
      "epoch: 1 [52160/60000 (87%)]\t training loss: 0.018086\n",
      "epoch: 1 [52480/60000 (87%)]\t training loss: 0.035340\n",
      "epoch: 1 [52800/60000 (88%)]\t training loss: 0.073316\n",
      "epoch: 1 [53120/60000 (89%)]\t training loss: 0.027918\n",
      "epoch: 1 [53440/60000 (89%)]\t training loss: 0.106017\n",
      "epoch: 1 [53760/60000 (90%)]\t training loss: 0.128013\n",
      "epoch: 1 [54080/60000 (90%)]\t training loss: 0.023136\n",
      "epoch: 1 [54400/60000 (91%)]\t training loss: 0.114827\n",
      "epoch: 1 [54720/60000 (91%)]\t training loss: 0.076415\n",
      "epoch: 1 [55040/60000 (92%)]\t training loss: 0.002461\n",
      "epoch: 1 [55360/60000 (92%)]\t training loss: 0.106343\n",
      "epoch: 1 [55680/60000 (93%)]\t training loss: 0.019520\n",
      "epoch: 1 [56000/60000 (93%)]\t training loss: 0.119551\n",
      "epoch: 1 [56320/60000 (94%)]\t training loss: 0.035312\n",
      "epoch: 1 [56640/60000 (94%)]\t training loss: 0.090167\n",
      "epoch: 1 [56960/60000 (95%)]\t training loss: 0.027890\n",
      "epoch: 1 [57280/60000 (95%)]\t training loss: 0.072046\n",
      "epoch: 1 [57600/60000 (96%)]\t training loss: 0.048883\n",
      "epoch: 1 [57920/60000 (97%)]\t training loss: 0.044264\n",
      "epoch: 1 [58240/60000 (97%)]\t training loss: 0.033600\n",
      "epoch: 1 [58560/60000 (98%)]\t training loss: 0.031268\n",
      "epoch: 1 [58880/60000 (98%)]\t training loss: 0.054833\n",
      "epoch: 1 [59200/60000 (99%)]\t training loss: 0.004903\n",
      "epoch: 1 [59520/60000 (99%)]\t training loss: 0.020394\n",
      "epoch: 1 [59840/60000 (100%)]\t training loss: 0.023926\n",
      "\n",
      "Test dataset: Overall Loss: 0.0484, Overall Accuracy: 9837/10000 (98%)\n",
      "\n",
      "epoch: 2 [0/60000 (0%)]\t training loss: 0.098174\n",
      "epoch: 2 [320/60000 (1%)]\t training loss: 0.023703\n",
      "epoch: 2 [640/60000 (1%)]\t training loss: 0.153959\n",
      "epoch: 2 [960/60000 (2%)]\t training loss: 0.136560\n",
      "epoch: 2 [1280/60000 (2%)]\t training loss: 0.049262\n",
      "epoch: 2 [1600/60000 (3%)]\t training loss: 0.021965\n",
      "epoch: 2 [1920/60000 (3%)]\t training loss: 0.027802\n",
      "epoch: 2 [2240/60000 (4%)]\t training loss: 0.003464\n",
      "epoch: 2 [2560/60000 (4%)]\t training loss: 0.006632\n",
      "epoch: 2 [2880/60000 (5%)]\t training loss: 0.077491\n",
      "epoch: 2 [3200/60000 (5%)]\t training loss: 0.009212\n",
      "epoch: 2 [3520/60000 (6%)]\t training loss: 0.162178\n",
      "epoch: 2 [3840/60000 (6%)]\t training loss: 0.057268\n",
      "epoch: 2 [4160/60000 (7%)]\t training loss: 0.165004\n",
      "epoch: 2 [4480/60000 (7%)]\t training loss: 0.000849\n",
      "epoch: 2 [4800/60000 (8%)]\t training loss: 0.055694\n",
      "epoch: 2 [5120/60000 (9%)]\t training loss: 0.187408\n",
      "epoch: 2 [5440/60000 (9%)]\t training loss: 0.154628\n",
      "epoch: 2 [5760/60000 (10%)]\t training loss: 0.009212\n",
      "epoch: 2 [6080/60000 (10%)]\t training loss: 0.171722\n",
      "epoch: 2 [6400/60000 (11%)]\t training loss: 0.079590\n",
      "epoch: 2 [6720/60000 (11%)]\t training loss: 0.163987\n",
      "epoch: 2 [7040/60000 (12%)]\t training loss: 0.090248\n",
      "epoch: 2 [7360/60000 (12%)]\t training loss: 0.040397\n",
      "epoch: 2 [7680/60000 (13%)]\t training loss: 0.077675\n",
      "epoch: 2 [8000/60000 (13%)]\t training loss: 0.027236\n",
      "epoch: 2 [8320/60000 (14%)]\t training loss: 0.035142\n",
      "epoch: 2 [8640/60000 (14%)]\t training loss: 0.091843\n",
      "epoch: 2 [8960/60000 (15%)]\t training loss: 0.440039\n",
      "epoch: 2 [9280/60000 (15%)]\t training loss: 0.086367\n",
      "epoch: 2 [9600/60000 (16%)]\t training loss: 0.024360\n",
      "epoch: 2 [9920/60000 (17%)]\t training loss: 0.006449\n",
      "epoch: 2 [10240/60000 (17%)]\t training loss: 0.010307\n",
      "epoch: 2 [10560/60000 (18%)]\t training loss: 0.107418\n",
      "epoch: 2 [10880/60000 (18%)]\t training loss: 0.022558\n",
      "epoch: 2 [11200/60000 (19%)]\t training loss: 0.045512\n",
      "epoch: 2 [11520/60000 (19%)]\t training loss: 0.035157\n",
      "epoch: 2 [11840/60000 (20%)]\t training loss: 0.014696\n",
      "epoch: 2 [12160/60000 (20%)]\t training loss: 0.016473\n",
      "epoch: 2 [12480/60000 (21%)]\t training loss: 0.039737\n",
      "epoch: 2 [12800/60000 (21%)]\t training loss: 0.049250\n",
      "epoch: 2 [13120/60000 (22%)]\t training loss: 0.157390\n",
      "epoch: 2 [13440/60000 (22%)]\t training loss: 0.018329\n",
      "epoch: 2 [13760/60000 (23%)]\t training loss: 0.014318\n",
      "epoch: 2 [14080/60000 (23%)]\t training loss: 0.052196\n",
      "epoch: 2 [14400/60000 (24%)]\t training loss: 0.034998\n",
      "epoch: 2 [14720/60000 (25%)]\t training loss: 0.043726\n",
      "epoch: 2 [15040/60000 (25%)]\t training loss: 0.037307\n",
      "epoch: 2 [15360/60000 (26%)]\t training loss: 0.126129\n",
      "epoch: 2 [15680/60000 (26%)]\t training loss: 0.014008\n",
      "epoch: 2 [16000/60000 (27%)]\t training loss: 0.017495\n",
      "epoch: 2 [16320/60000 (27%)]\t training loss: 0.027524\n",
      "epoch: 2 [16640/60000 (28%)]\t training loss: 0.080501\n",
      "epoch: 2 [16960/60000 (28%)]\t training loss: 0.011050\n",
      "epoch: 2 [17280/60000 (29%)]\t training loss: 0.060049\n",
      "epoch: 2 [17600/60000 (29%)]\t training loss: 0.101983\n",
      "epoch: 2 [17920/60000 (30%)]\t training loss: 0.000665\n",
      "epoch: 2 [18240/60000 (30%)]\t training loss: 0.074447\n",
      "epoch: 2 [18560/60000 (31%)]\t training loss: 0.013062\n",
      "epoch: 2 [18880/60000 (31%)]\t training loss: 0.014776\n",
      "epoch: 2 [19200/60000 (32%)]\t training loss: 0.010192\n",
      "epoch: 2 [19520/60000 (33%)]\t training loss: 0.009810\n",
      "epoch: 2 [19840/60000 (33%)]\t training loss: 0.169672\n",
      "epoch: 2 [20160/60000 (34%)]\t training loss: 0.012401\n",
      "epoch: 2 [20480/60000 (34%)]\t training loss: 0.004820\n",
      "epoch: 2 [20800/60000 (35%)]\t training loss: 0.015489\n",
      "epoch: 2 [21120/60000 (35%)]\t training loss: 0.077651\n",
      "epoch: 2 [21440/60000 (36%)]\t training loss: 0.033374\n",
      "epoch: 2 [21760/60000 (36%)]\t training loss: 0.022966\n",
      "epoch: 2 [22080/60000 (37%)]\t training loss: 0.073334\n",
      "epoch: 2 [22400/60000 (37%)]\t training loss: 0.000803\n",
      "epoch: 2 [22720/60000 (38%)]\t training loss: 0.006425\n",
      "epoch: 2 [23040/60000 (38%)]\t training loss: 0.016063\n",
      "epoch: 2 [23360/60000 (39%)]\t training loss: 0.004546\n",
      "epoch: 2 [23680/60000 (39%)]\t training loss: 0.003831\n",
      "epoch: 2 [24000/60000 (40%)]\t training loss: 0.047946\n",
      "epoch: 2 [24320/60000 (41%)]\t training loss: 0.002812\n",
      "epoch: 2 [24640/60000 (41%)]\t training loss: 0.032683\n",
      "epoch: 2 [24960/60000 (42%)]\t training loss: 0.079832\n",
      "epoch: 2 [25280/60000 (42%)]\t training loss: 0.015188\n",
      "epoch: 2 [25600/60000 (43%)]\t training loss: 0.009036\n",
      "epoch: 2 [25920/60000 (43%)]\t training loss: 0.002251\n",
      "epoch: 2 [26240/60000 (44%)]\t training loss: 0.108962\n",
      "epoch: 2 [26560/60000 (44%)]\t training loss: 0.073499\n",
      "epoch: 2 [26880/60000 (45%)]\t training loss: 0.065956\n",
      "epoch: 2 [27200/60000 (45%)]\t training loss: 0.008357\n",
      "epoch: 2 [27520/60000 (46%)]\t training loss: 0.007351\n",
      "epoch: 2 [27840/60000 (46%)]\t training loss: 0.033639\n",
      "epoch: 2 [28160/60000 (47%)]\t training loss: 0.153226\n",
      "epoch: 2 [28480/60000 (47%)]\t training loss: 0.044676\n",
      "epoch: 2 [28800/60000 (48%)]\t training loss: 0.011117\n",
      "epoch: 2 [29120/60000 (49%)]\t training loss: 0.074001\n",
      "epoch: 2 [29440/60000 (49%)]\t training loss: 0.193947\n",
      "epoch: 2 [29760/60000 (50%)]\t training loss: 0.043343\n",
      "epoch: 2 [30080/60000 (50%)]\t training loss: 0.002562\n",
      "epoch: 2 [30400/60000 (51%)]\t training loss: 0.042341\n",
      "epoch: 2 [30720/60000 (51%)]\t training loss: 0.373437\n",
      "epoch: 2 [31040/60000 (52%)]\t training loss: 0.007059\n",
      "epoch: 2 [31360/60000 (52%)]\t training loss: 0.028347\n",
      "epoch: 2 [31680/60000 (53%)]\t training loss: 0.066970\n",
      "epoch: 2 [32000/60000 (53%)]\t training loss: 0.201568\n",
      "epoch: 2 [32320/60000 (54%)]\t training loss: 0.024534\n",
      "epoch: 2 [32640/60000 (54%)]\t training loss: 0.104064\n",
      "epoch: 2 [32960/60000 (55%)]\t training loss: 0.009888\n",
      "epoch: 2 [33280/60000 (55%)]\t training loss: 0.007596\n",
      "epoch: 2 [33600/60000 (56%)]\t training loss: 0.397131\n",
      "epoch: 2 [33920/60000 (57%)]\t training loss: 0.070473\n",
      "epoch: 2 [34240/60000 (57%)]\t training loss: 0.033654\n",
      "epoch: 2 [34560/60000 (58%)]\t training loss: 0.019546\n",
      "epoch: 2 [34880/60000 (58%)]\t training loss: 0.009284\n",
      "epoch: 2 [35200/60000 (59%)]\t training loss: 0.035103\n",
      "epoch: 2 [35520/60000 (59%)]\t training loss: 0.008294\n",
      "epoch: 2 [35840/60000 (60%)]\t training loss: 0.007669\n",
      "epoch: 2 [36160/60000 (60%)]\t training loss: 0.095953\n",
      "epoch: 2 [36480/60000 (61%)]\t training loss: 0.079869\n",
      "epoch: 2 [36800/60000 (61%)]\t training loss: 0.037694\n",
      "epoch: 2 [37120/60000 (62%)]\t training loss: 0.267672\n",
      "epoch: 2 [37440/60000 (62%)]\t training loss: 0.108975\n",
      "epoch: 2 [37760/60000 (63%)]\t training loss: 0.139832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 [38080/60000 (63%)]\t training loss: 0.096193\n",
      "epoch: 2 [38400/60000 (64%)]\t training loss: 0.117113\n",
      "epoch: 2 [38720/60000 (65%)]\t training loss: 0.014235\n",
      "epoch: 2 [39040/60000 (65%)]\t training loss: 0.014314\n",
      "epoch: 2 [39360/60000 (66%)]\t training loss: 0.034373\n",
      "epoch: 2 [39680/60000 (66%)]\t training loss: 0.001104\n",
      "epoch: 2 [40000/60000 (67%)]\t training loss: 0.147021\n",
      "epoch: 2 [40320/60000 (67%)]\t training loss: 0.058104\n",
      "epoch: 2 [40640/60000 (68%)]\t training loss: 0.012090\n",
      "epoch: 2 [40960/60000 (68%)]\t training loss: 0.002220\n",
      "epoch: 2 [41280/60000 (69%)]\t training loss: 0.024926\n",
      "epoch: 2 [41600/60000 (69%)]\t training loss: 0.018475\n",
      "epoch: 2 [41920/60000 (70%)]\t training loss: 0.002290\n",
      "epoch: 2 [42240/60000 (70%)]\t training loss: 0.000232\n",
      "epoch: 2 [42560/60000 (71%)]\t training loss: 0.014631\n",
      "epoch: 2 [42880/60000 (71%)]\t training loss: 0.024298\n",
      "epoch: 2 [43200/60000 (72%)]\t training loss: 0.030139\n",
      "epoch: 2 [43520/60000 (73%)]\t training loss: 0.087239\n",
      "epoch: 2 [43840/60000 (73%)]\t training loss: 0.007357\n",
      "epoch: 2 [44160/60000 (74%)]\t training loss: 0.150379\n",
      "epoch: 2 [44480/60000 (74%)]\t training loss: 0.020359\n",
      "epoch: 2 [44800/60000 (75%)]\t training loss: 0.015349\n",
      "epoch: 2 [45120/60000 (75%)]\t training loss: 0.002177\n",
      "epoch: 2 [45440/60000 (76%)]\t training loss: 0.148906\n",
      "epoch: 2 [45760/60000 (76%)]\t training loss: 0.134503\n",
      "epoch: 2 [46080/60000 (77%)]\t training loss: 0.170075\n",
      "epoch: 2 [46400/60000 (77%)]\t training loss: 0.262906\n",
      "epoch: 2 [46720/60000 (78%)]\t training loss: 0.034090\n",
      "epoch: 2 [47040/60000 (78%)]\t training loss: 0.004239\n",
      "epoch: 2 [47360/60000 (79%)]\t training loss: 0.013591\n",
      "epoch: 2 [47680/60000 (79%)]\t training loss: 0.131799\n",
      "epoch: 2 [48000/60000 (80%)]\t training loss: 0.221314\n",
      "epoch: 2 [48320/60000 (81%)]\t training loss: 0.021087\n",
      "epoch: 2 [48640/60000 (81%)]\t training loss: 0.016724\n",
      "epoch: 2 [48960/60000 (82%)]\t training loss: 0.163884\n",
      "epoch: 2 [49280/60000 (82%)]\t training loss: 0.095761\n",
      "epoch: 2 [49600/60000 (83%)]\t training loss: 0.103023\n",
      "epoch: 2 [49920/60000 (83%)]\t training loss: 0.031056\n",
      "epoch: 2 [50240/60000 (84%)]\t training loss: 0.066328\n",
      "epoch: 2 [50560/60000 (84%)]\t training loss: 0.008336\n",
      "epoch: 2 [50880/60000 (85%)]\t training loss: 0.373142\n",
      "epoch: 2 [51200/60000 (85%)]\t training loss: 0.002989\n",
      "epoch: 2 [51520/60000 (86%)]\t training loss: 0.041172\n",
      "epoch: 2 [51840/60000 (86%)]\t training loss: 0.500364\n",
      "epoch: 2 [52160/60000 (87%)]\t training loss: 0.022897\n",
      "epoch: 2 [52480/60000 (87%)]\t training loss: 0.046868\n",
      "epoch: 2 [52800/60000 (88%)]\t training loss: 0.269853\n",
      "epoch: 2 [53120/60000 (89%)]\t training loss: 0.009009\n",
      "epoch: 2 [53440/60000 (89%)]\t training loss: 0.136329\n",
      "epoch: 2 [53760/60000 (90%)]\t training loss: 0.073011\n",
      "epoch: 2 [54080/60000 (90%)]\t training loss: 0.025033\n",
      "epoch: 2 [54400/60000 (91%)]\t training loss: 0.156014\n",
      "epoch: 2 [54720/60000 (91%)]\t training loss: 0.365546\n",
      "epoch: 2 [55040/60000 (92%)]\t training loss: 0.031742\n",
      "epoch: 2 [55360/60000 (92%)]\t training loss: 0.015810\n",
      "epoch: 2 [55680/60000 (93%)]\t training loss: 0.236210\n",
      "epoch: 2 [56000/60000 (93%)]\t training loss: 0.032649\n",
      "epoch: 2 [56320/60000 (94%)]\t training loss: 0.189793\n",
      "epoch: 2 [56640/60000 (94%)]\t training loss: 0.456181\n",
      "epoch: 2 [56960/60000 (95%)]\t training loss: 0.004161\n",
      "epoch: 2 [57280/60000 (95%)]\t training loss: 0.021010\n",
      "epoch: 2 [57600/60000 (96%)]\t training loss: 0.122520\n",
      "epoch: 2 [57920/60000 (97%)]\t training loss: 0.014843\n",
      "epoch: 2 [58240/60000 (97%)]\t training loss: 0.077786\n",
      "epoch: 2 [58560/60000 (98%)]\t training loss: 0.008636\n",
      "epoch: 2 [58880/60000 (98%)]\t training loss: 0.004934\n",
      "epoch: 2 [59200/60000 (99%)]\t training loss: 0.211395\n",
      "epoch: 2 [59520/60000 (99%)]\t training loss: 0.087741\n",
      "epoch: 2 [59840/60000 (100%)]\t training loss: 0.001973\n",
      "\n",
      "Test dataset: Overall Loss: 0.0408, Overall Accuracy: 9854/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 3):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run inference on trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_samples = enumerate(test_dataloader)\n",
    "b_i, (sample_data, sample_targets) = next(test_samples)\n",
    "\n",
    "plt.imshow(sample_data[0][0], cmap='gray', interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction is : 7\n",
      "Ground truth is : 7\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model prediction is : {model(sample_data).data.max(1)[1][0]}\")\n",
    "print(f\"Ground truth is : {sample_targets[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
